{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "# Load API keys\n",
    "load_dotenv()\n",
    "BASE_URL = \"https://twitter241.p.rapidapi.com\"\n",
    "HEADERS = {\n",
    "    \"x-rapidapi-host\": \"twitter241.p.rapidapi.com\",\n",
    "    \"x-rapidapi-key\": \"056375d7f5mshd4546f4fb1a7f4ep129c82jsna85290da7d03\"\n",
    "}\n",
    "\n",
    "Influencer list\n",
    "INFLUENCER_USERNAMES = [\n",
    "    \"Ninja\", \"shroud\", \"Myth_\", \"DrLupo\", \"TimTheTatman\", \"Syndicate\", \"Summit1g\", \"Pokimane\",\n",
    "    \"Tfue\", \"Jacksepticeye\", \"Valkyrae\", \"Quackity\", \"TheGrefg\", \"Jynxzi\", \"markiplier\",\n",
    "    \"SSSniperWolf\", \"OMGitsAliA\", \"scump\", \"LazarBeam\", \"Pokelawls\"\n",
    "]\n",
    "\n",
    "\n",
    "# Retry decorator for 429 errors\n",
    "# Retry decorator: infinite retries with no delay\n",
    "\n",
    "\n",
    "def retry_on_429():\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except HTTPError as e:\n",
    "                    if e.response.status_code == 429:\n",
    "                        print(f\"⏳ 429 Too Many Requests. Retrying in 0.5s...\")\n",
    "                        time.sleep(0.5)\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "\n",
    "# API calls\n",
    "@retry_on_429()\n",
    "def get_user_info(username):\n",
    "    url = f\"{BASE_URL}/user\"\n",
    "    params = {\"username\": username}\n",
    "    response = requests.get(url, headers=HEADERS, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "@retry_on_429()\n",
    "def get_user_followers(user_id, count=100):\n",
    "    url = f\"{BASE_URL}/followers\"\n",
    "    params = {\"user\": user_id, \"count\": count}\n",
    "    response = requests.get(url, headers=HEADERS, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "@retry_on_429()\n",
    "def get_user_following(user_id, count=100):\n",
    "    url = f\"{BASE_URL}/followings\"\n",
    "    params = {\"user\": user_id, \"count\": count}\n",
    "    response = requests.get(url, headers=HEADERS, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "@retry_on_429()\n",
    "def get_user_tweets(user_id, count=50):\n",
    "    url = f\"{BASE_URL}/user-tweets\"\n",
    "    params = {\"user\": user_id, \"count\": count}\n",
    "    response = requests.get(url, headers=HEADERS, params=params)\n",
    "    response.raise_for_status()\n",
    "    return response.json()\n",
    "\n",
    "# Output folders\n",
    "os.makedirs(\"data/raw/profiles\", exist_ok=True)\n",
    "os.makedirs(\"data/raw/followers\", exist_ok=True)\n",
    "os.makedirs(\"data/raw/following\", exist_ok=True)\n",
    "os.makedirs(\"data/raw/tweets\", exist_ok=True)\n",
    "\n",
    "# Collect data\n",
    "for username in INFLUENCER_USERNAMES:\n",
    "    print(f\"\\n📦 Collecting data for {username}...\")\n",
    "\n",
    "    user_info = get_user_info(username)\n",
    "    if not user_info:\n",
    "        print(f\"⚠️ Failed to fetch user info for {username}\")\n",
    "        continue\n",
    "\n",
    "    # Extract user ID from nested structure\n",
    "    user_data = user_info.get(\"result\", {}).get(\"data\", {}).get(\"user\", {}).get(\"result\", {})\n",
    "    user_id = user_data.get(\"rest_id\")\n",
    "\n",
    "    if not user_id:\n",
    "        print(f\"❌ User ID not found for {username}\")\n",
    "        continue\n",
    "\n",
    "    # Save profile\n",
    "    with open(f\"data/raw/profiles/{username}.json\", \"w\") as f:\n",
    "        json.dump(user_info, f, indent=2)\n",
    "    print(f\"✅ Profile saved for {username}\")\n",
    "\n",
    "    # Get and save tweets\n",
    "    tweets = get_user_tweets(user_id)\n",
    "    if tweets:\n",
    "        with open(f\"data/raw/tweets/{username}.json\", \"w\") as f:\n",
    "            json.dump([{\"tweets\": tweets}], f, indent=2)\n",
    "        print(f\"✅ Tweets saved for {username}\")\n",
    "\n",
    "    # Get and save followers\n",
    "    followers = get_user_followers(user_id)\n",
    "    if followers:\n",
    "        with open(f\"data/raw/followers/{username}.json\", \"w\") as f:\n",
    "            json.dump(followers, f, indent=2)\n",
    "        print(f\"✅ Followers saved for {username}\")\n",
    "\n",
    "    # Get and save following\n",
    "    following = get_user_following(user_id)\n",
    "    if following:\n",
    "        with open(f\"data/raw/following/{username}.json\", \"w\") as f:\n",
    "            json.dump(following, f, indent=2)\n",
    "        print(f\"✅ Following saved for {username}\")\n",
    "\n",
    "    # Delay to reduce risk of rate limits\n",
    "\n",
    "print(\"\\n🎉 All data collected and saved per influencer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b36a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_json(json_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Extracts and aggregates tweet and profile features from an influencer's tweet JSON file.\"\"\"\n",
    "    \n",
    "    # Load tweet JSON\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Extract tweets\n",
    "    entries = data[0]['tweets']['result']['timeline']['instructions']\n",
    "    tweet_items = []\n",
    "    for instruction in entries:\n",
    "        if instruction.get('type') == \"TimelineAddEntries\":\n",
    "            for entry in instruction['entries']:\n",
    "                try:\n",
    "                    tweet = entry['content']['itemContent']['tweet_results']['result']\n",
    "                    tweet_items.append(tweet)\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "    # Helper: parse timestamp\n",
    "    def parse_datetime(t): return datetime.strptime(t, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "\n",
    "    # Helper: sentiment\n",
    "    def get_sentiment(text):\n",
    "        blob = TextBlob(text)\n",
    "        return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "    # Extract features per tweet\n",
    "    records = []\n",
    "    for t in tweet_items:\n",
    "        legacy = t['legacy']\n",
    "        views = t.get('views', {}).get('count', '0')\n",
    "        text = legacy.get('full_text', '')\n",
    "        media = legacy.get('entities', {}).get('media', [])\n",
    "        urls = legacy.get('entities', {}).get('urls', [])\n",
    "        mentions = legacy.get('entities', {}).get('user_mentions', [])\n",
    "        hashtags = legacy.get('entities', {}).get('hashtags', [])\n",
    "        polarity, subjectivity = get_sentiment(text)\n",
    "\n",
    "        views_count = int(views.replace(',', '')) if views else 1\n",
    "        total_engagement = legacy['favorite_count'] + legacy['retweet_count'] + legacy['reply_count']\n",
    "        engagement_rate = total_engagement / views_count if views_count > 0 else 0\n",
    "\n",
    "        records.append({\n",
    "            \"created_at\": parse_datetime(legacy['created_at']),\n",
    "            \"text_length\": len(text),\n",
    "            \"word_count\": len(re.findall(r'\\w+', text)),\n",
    "            \"has_media\": int(bool(media)),\n",
    "            \"has_url\": int(bool(urls)),\n",
    "            \"has_mentions\": int(bool(mentions)),\n",
    "            \"has_hashtags\": int(bool(hashtags)),\n",
    "            \"favorite_count\": legacy['favorite_count'],\n",
    "            \"retweet_count\": legacy['retweet_count'],\n",
    "            \"reply_count\": legacy['reply_count'],\n",
    "            \"bookmark_count\": legacy.get('bookmark_count', 0),\n",
    "            \"views\": views_count,\n",
    "            \"engagement_rate\": engagement_rate,\n",
    "            \"sentiment_polarity\": polarity,\n",
    "            \"sentiment_subjectivity\": subjectivity,\n",
    "            \"hour\": parse_datetime(legacy['created_at']).hour,\n",
    "            \"weekday\": parse_datetime(legacy['created_at']).weekday(),\n",
    "            \"is_weekend\": int(parse_datetime(legacy['created_at']).weekday() >= 5),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No valid tweets found in JSON.\")\n",
    "\n",
    "    # === Aggregation ===\n",
    "    agg_features = {\n",
    "        \"text_length\": [\"mean\", \"std\", \"max\"],\n",
    "        \"word_count\": [\"mean\", \"std\", \"max\"],\n",
    "        \"has_media\": \"mean\",\n",
    "        \"has_url\": \"mean\",\n",
    "        \"has_mentions\": \"mean\",\n",
    "        \"has_hashtags\": \"mean\",\n",
    "        \"favorite_count\": [\"mean\", \"max\"],\n",
    "        \"retweet_count\": [\"mean\", \"max\"],\n",
    "        \"reply_count\": [\"mean\", \"max\"],\n",
    "        \"bookmark_count\": [\"mean\", \"max\"],\n",
    "        \"views\": [\"mean\", \"max\"],\n",
    "        \"engagement_rate\": [\"mean\", \"max\", \"std\"],\n",
    "        \"sentiment_polarity\": [\"mean\", \"std\"],\n",
    "        \"sentiment_subjectivity\": [\"mean\", \"std\"],\n",
    "        \"hour\": [\"mean\"],\n",
    "        \"is_weekend\": \"mean\"\n",
    "    }\n",
    "\n",
    "    agg_df = df.agg(agg_features)\n",
    "    agg_df.columns = ['{}_{}'.format(col[0], col[1]) if isinstance(col, tuple) else col for col in agg_df.columns]\n",
    "\n",
    "    # === Add user profile features ===\n",
    "    user_info = tweet_items[0]['core']['user_results']['result']['legacy']\n",
    "    created_at_user = parse_datetime(user_info['created_at'])\n",
    "    profile_features = {\n",
    "        \"followers_count\": user_info[\"followers_count\"],\n",
    "        \"friends_count\": user_info[\"friends_count\"],\n",
    "        \"listed_count\": user_info[\"listed_count\"],\n",
    "        \"statuses_count\": user_info[\"statuses_count\"],\n",
    "        \"media_count\": user_info[\"media_count\"],\n",
    "        \"favourites_count\": user_info[\"favourites_count\"],\n",
    "        \"account_age_days\": (datetime.utcnow() - created_at_user.replace(tzinfo=None)).days,\n",
    "        \"follower_following_ratio\": user_info[\"followers_count\"] / (user_info[\"friends_count\"] + 1)\n",
    "    }\n",
    "\n",
    "    final_features = pd.concat([agg_df.reset_index(drop=True), pd.DataFrame([profile_features])], axis=1)\n",
    "    return final_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a10cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_dir = \"data/raw/tweets/\"\n",
    "output_dir = \"data/outputs/engagement_features/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith(\".json\"):\n",
    "        screen_name = file.replace(\".json\", \"\")\n",
    "        try:\n",
    "            feature_df = extract_features_from_json(os.path.join(input_dir, file))\n",
    "            feature_df.to_csv(f\"{output_dir}/{screen_name}_features.csv\", index=False)\n",
    "            print(f\"✅ Saved: {screen_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {screen_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a240935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load centrality data\n",
    "centrality_df = pd.read_csv(\"data/outputs/centrality/influencer_centrality.csv\")\n",
    "\n",
    "# Drop redundant profile fields (to avoid overlap with final_features)\n",
    "redundant_cols = ['followers_count', 'friends_count', 'statuses_count', 'name', 'screen_name']\n",
    "\n",
    "# List of influencer Twitter handles\n",
    "INFLUENCER_USERNAMES = [\n",
    "    \"Ninja\", \"shroud\", \"Myth_\", \"DrLupo\", \"TimTheTatman\", \"Syndicate\", \"Summit1g\", \"Pokimane\",\n",
    "    \"Tfue\", \"Jacksepticeye\", \"Valkyrae\", \"Quackity\", \"TheGrefg\", \"Jynxzi\", \"markiplier\",\n",
    "    \"SSSniperWolf\", \"OMGitsAliA\", \"scump\", \"LazarBeam\", \"Pokelawls\"\n",
    "]\n",
    "\n",
    "# Placeholder to store all merged datasets\n",
    "all_merged = []\n",
    "\n",
    "for influencer_screen_name in INFLUENCER_USERNAMES:\n",
    "    # Load or generate final_features for this influencer\n",
    "    # Replace this with your actual logic\n",
    "    try:\n",
    "        final_features = pd.read_csv(f\"data/outputs/engagement_features/{influencer_screen_name}_features.csv\")\n",
    "        final_features = final_features.loc[:, ~final_features.columns.str.contains('^Unnamed')]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ Skipping {influencer_screen_name}: features file not found.\")\n",
    "        continue\n",
    "\n",
    "    # Match centrality row\n",
    "    influencer_centrality = centrality_df[\n",
    "        centrality_df['screen_name'].str.lower() == influencer_screen_name.lower()\n",
    "    ].reset_index(drop=True)\n",
    "\n",
    "    if influencer_centrality.empty:\n",
    "        print(f\"⚠️ Skipping {influencer_screen_name}: no centrality data found.\")\n",
    "        continue\n",
    "\n",
    "    influencer_centrality = influencer_centrality.drop(columns=redundant_cols, errors='ignore')\n",
    "\n",
    "    # Keep only \"mean\" row if final_features has multiple rows (mean/std/max)\n",
    "    if \"mean\" in final_features.index or final_features.shape[0] > 1:\n",
    "        try:\n",
    "            final_features = final_features.set_index(\"stat\").loc[[\"mean\"]].reset_index(drop=True)\n",
    "        except:\n",
    "            final_features = final_features.iloc[[0]]  # fallback to first row\n",
    "    else:\n",
    "        final_features = final_features.iloc[[0]]\n",
    "\n",
    "    # Merge horizontally\n",
    "    merged = pd.concat([final_features.reset_index(drop=True), influencer_centrality], axis=1)\n",
    "    merged[\"screen_name\"] = influencer_screen_name  # Add label for traceability\n",
    "\n",
    "    print(f\"\\n✅ Merged dataset for {influencer_screen_name}:\")\n",
    "    print(merged.T)\n",
    "\n",
    "    all_merged.append(merged)\n",
    "\n",
    "# Optionally combine all into one DataFrame\n",
    "final_dataset = pd.concat(all_merged, ignore_index=True)\n",
    "\n",
    "# Save merged dataset\n",
    "final_dataset.to_csv(\"data/outputs/influencer_features_combined_all.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b308ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "def parse_datetime(date_str: str) -> datetime | None:\n",
    "    if not date_str:\n",
    "        return None\n",
    "    return datetime.strptime(date_str, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "\n",
    "\n",
    "def get_sentiment(text: str) -> tuple[float, float]:\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "\n",
    "def extract_tweet_features(json_path: str) -> pd.DataFrame:\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    tweet_records = []\n",
    "    try:\n",
    "        instructions = data['tweets']['result']['timeline']['instructions']\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Invalid JSON structure: Missing 'instructions'.\")\n",
    "\n",
    "    entries = []\n",
    "    for instr in instructions:\n",
    "        if instr.get(\"type\") == \"TimelineAddEntries\":\n",
    "            entries.extend(instr.get(\"entries\", []))\n",
    "        elif instr.get(\"entry\", {}).get(\"content\", {}).get(\"itemContent\"):\n",
    "            entries.append(instr[\"entry\"])  # Handle pinned tweets\n",
    "\n",
    "    for entry in entries:\n",
    "        content = entry.get(\"content\", {})\n",
    "        item_content = content.get(\"itemContent\", {})\n",
    "        tweet_result = item_content.get(\"tweet_results\", {}).get(\"result\", {})\n",
    "\n",
    "        if tweet_result.get(\"__typename\") != \"Tweet\":\n",
    "            continue\n",
    "\n",
    "        legacy = tweet_result.get(\"legacy\", {})\n",
    "        user_legacy = tweet_result.get(\"core\", {}).get(\"user_results\", {}).get(\"result\", {}).get(\"legacy\", {})\n",
    "\n",
    "        full_text = legacy.get(\"full_text\", \"\")\n",
    "        polarity, subjectivity = get_sentiment(full_text)\n",
    "\n",
    "        tweet_info = {\n",
    "            \"tweet_id\": legacy.get(\"id_str\"),\n",
    "            \"created_at\": parse_datetime(legacy.get(\"created_at\")),\n",
    "            \"full_text\": full_text,\n",
    "            \"like_count\": legacy.get(\"favorite_count\"),\n",
    "            \"retweet_count\": legacy.get(\"retweet_count\"),\n",
    "            \"reply_count\": legacy.get(\"reply_count\"),\n",
    "            \"quote_count\": legacy.get(\"quote_count\"),\n",
    "            \"view_count\": int(tweet_result.get(\"views\", {}).get(\"count\", 0)),\n",
    "            \"text_length\": len(full_text),\n",
    "            \"word_count\": len(full_text.split()),\n",
    "            \"has_media\": int(\"media\" in legacy),\n",
    "            \"has_url\": int(bool(legacy.get(\"urls\"))),\n",
    "            \"has_mentions\": int(bool(legacy.get(\"user_mentions\"))),\n",
    "            \"has_hashtags\": int(bool(legacy.get(\"hashtags\"))),\n",
    "            \"sentiment_polarity\": polarity,\n",
    "            \"sentiment_subjectivity\": subjectivity,\n",
    "            \"hour\": parse_datetime(legacy.get(\"created_at\")).hour if legacy.get(\"created_at\") else None,\n",
    "            \"is_weekend\": int(parse_datetime(legacy.get(\"created_at\")).weekday() >= 5) if legacy.get(\"created_at\") else None,\n",
    "            \"engagement_rate\": (\n",
    "                legacy.get(\"favorite_count\", 0) +\n",
    "                legacy.get(\"retweet_count\", 0) +\n",
    "                legacy.get(\"reply_count\", 0) +\n",
    "                legacy.get(\"quote_count\", 0)\n",
    "            ) / (int(tweet_result.get(\"views\", {}).get(\"count\", 1))),\n",
    "            \"_user_legacy\": user_legacy,\n",
    "        }\n",
    "\n",
    "        tweet_records.append(tweet_info)\n",
    "\n",
    "    df = pd.DataFrame(tweet_records)\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No valid tweets found in {json_path}\")\n",
    "    df = df.sort_values(\"created_at\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def aggregate_features(tweet_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    agg_map = {\n",
    "        \"text_length\": [\"mean\", \"std\", \"max\"],\n",
    "        \"word_count\": [\"mean\", \"std\", \"max\"],\n",
    "        \"has_media\": \"mean\",\n",
    "        \"has_url\": \"mean\",\n",
    "        \"has_mentions\": \"mean\",\n",
    "        \"has_hashtags\": \"mean\",\n",
    "        \"like_count\": [\"mean\", \"max\"],\n",
    "        \"retweet_count\": [\"mean\", \"max\"],\n",
    "        \"reply_count\": [\"mean\", \"max\"],\n",
    "        \"quote_count\": [\"mean\", \"max\"],\n",
    "        \"view_count\": [\"mean\", \"max\"],\n",
    "        \"engagement_rate\": [\"mean\", \"max\", \"std\"],\n",
    "        \"sentiment_polarity\": [\"mean\", \"std\"],\n",
    "        \"sentiment_subjectivity\": [\"mean\", \"std\"],\n",
    "        \"hour\": \"mean\",\n",
    "        \"is_weekend\": \"mean\"\n",
    "    }\n",
    "\n",
    "    agg_df = tweet_df.agg(agg_map)\n",
    "    agg_df.columns = ['{}_{}'.format(col[0], col[1]) if isinstance(col, tuple) else col for col in agg_df.columns]\n",
    "\n",
    "    user_info = tweet_df.iloc[0][\"_user_legacy\"]\n",
    "    created_at_user = parse_datetime(user_info.get(\"created_at\"))\n",
    "    account_age = (datetime.utcnow() - created_at_user.replace(tzinfo=None)).days if created_at_user else None\n",
    "\n",
    "    profile_features = {\n",
    "        \"followers_count\": user_info.get(\"followers_count\", 0),\n",
    "        \"friends_count\": user_info.get(\"friends_count\", 0),\n",
    "        \"listed_count\": user_info.get(\"listed_count\", 0),\n",
    "        \"statuses_count\": user_info.get(\"statuses_count\", 0),\n",
    "        \"media_count\": user_info.get(\"media_count\", 0),\n",
    "        \"favourites_count\": user_info.get(\"favourites_count\", 0),\n",
    "        \"account_age_days\": account_age,\n",
    "        \"follower_following_ratio\": user_info.get(\"followers_count\", 0) / (user_info.get(\"friends_count\", 0) + 1)\n",
    "    }\n",
    "\n",
    "    final_features = pd.concat(\n",
    "        [agg_df.reset_index(drop=True), pd.DataFrame([profile_features])], axis=1\n",
    "    )\n",
    "    return final_features\n",
    "\n",
    "\n",
    "def load_multiple_influencers(json_paths: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and concatenate tweet-level features from multiple influencers.\n",
    "    json_paths: dict {influencer_id: json_file_path}\n",
    "    Returns concatenated DataFrame with influencer_id column.\n",
    "    \"\"\"\n",
    "    all_tweets = []\n",
    "    failed = []\n",
    "\n",
    "    for influencer_id, path in json_paths.items():\n",
    "        try:\n",
    "            df = extract_tweet_features(path)\n",
    "            df['influencer_id'] = influencer_id\n",
    "            all_tweets.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to process {influencer_id} at {path}: {e}\")\n",
    "            failed.append(influencer_id)\n",
    "\n",
    "    if not all_tweets:\n",
    "        raise ValueError(\"No valid tweet data extracted. Check if JSON structure has changed or files are empty.\")\n",
    "\n",
    "    print(f\"✅ Successfully processed {len(all_tweets)} influencers.\")\n",
    "    if failed:\n",
    "        print(f\"⚠️ Failed to process {len(failed)} influencers: {failed}\")\n",
    "\n",
    "    return pd.concat(all_tweets, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "def train_test_split_time_based(df: pd.DataFrame, split_ratio=0.8) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    train_list, test_list = [], []\n",
    "    for influencer_id, group in df.groupby(\"influencer_id\"):\n",
    "        group_sorted = group.sort_values(\"created_at\")\n",
    "        split_idx = int(len(group_sorted) * split_ratio)\n",
    "        train_list.append(group_sorted.iloc[:split_idx])\n",
    "        test_list.append(group_sorted.iloc[split_idx:])\n",
    "    return pd.concat(train_list).reset_index(drop=True), pd.concat(test_list).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677fc4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"data/raw/tweets/\"\n",
    "output_dir = \"data/outputs/engagement_features/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Auto-generate json_paths dict: {screen_name: full_path}\n",
    "json_paths = {\n",
    "    file.replace(\".json\", \"\"): os.path.join(input_dir, file)\n",
    "    for file in os.listdir(input_dir)\n",
    "    if file.endswith(\".json\")\n",
    "}\n",
    "\n",
    "print(f\"Found {len(json_paths)} influencer JSON files.\")\n",
    "\n",
    "# Load all influencers' tweet data into one DataFrame\n",
    "print(json_paths)\n",
    "all_tweets_df = load_multiple_influencers(json_paths)\n",
    "\n",
    "print(f\"Loaded tweets for {all_tweets_df['influencer_id'].nunique()} influencers, total {len(all_tweets_df)} tweets.\")\n",
    "\n",
    "# Split into train/test sets by time (80% train, 20% test)\n",
    "train_df, test_df = train_test_split_time_based(all_tweets_df, split_ratio=0.8)\n",
    "\n",
    "print(f\"Train set: {len(train_df)} tweets, Test set: {len(test_df)} tweets.\")\n",
    "\n",
    "# Aggregate features per influencer (using all tweets)\n",
    "agg_features_list = []\n",
    "for influencer_id in all_tweets_df['influencer_id'].unique():\n",
    "    inf_tweets = all_tweets_df[all_tweets_df['influencer_id'] == influencer_id]\n",
    "    agg_df = aggregate_features(inf_tweets)\n",
    "    agg_df['influencer_id'] = influencer_id\n",
    "    agg_features_list.append(agg_df)\n",
    "\n",
    "agg_features_df = pd.concat(agg_features_list, ignore_index=True)\n",
    "\n",
    "# Save aggregated features to CSV\n",
    "output_path = os.path.join(output_dir, \"aggregated_engagement_features.csv\")\n",
    "agg_features_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved aggregated features for {len(agg_features_df)} influencers to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wif3009",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
