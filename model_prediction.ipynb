{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b78e81",
   "metadata": {},
   "source": [
    "## Load and Extract Tweet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def load_data(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def extract_tweet_features(data):\n",
    "    all_tweets = []\n",
    "\n",
    "    for influencer in data:\n",
    "        user_id = influencer[\"user_id\"]\n",
    "        username = influencer[\"username\"]\n",
    "        tweets = influencer.get(\"tweets\", {}).get(\"result\", {}).get(\"timeline\", {}).get(\"instructions\", [])\n",
    "\n",
    "        for instr in tweets:\n",
    "            if instr.get(\"type\") == \"TimelineAddEntries\":\n",
    "                for entry in instr.get(\"entries\", []):\n",
    "                    try:\n",
    "                        tweet = entry[\"content\"][\"itemContent\"][\"tweet_results\"][\"result\"]\n",
    "                        legacy = tweet[\"legacy\"]\n",
    "                        created_at = legacy[\"created_at\"]\n",
    "                        dt = datetime.strptime(created_at, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "\n",
    "                        # Engagement features\n",
    "                        tweet_features = {\n",
    "                            \"user_id\": user_id,\n",
    "                            \"username\": username,\n",
    "                            \"tweet_id\": tweet[\"rest_id\"],\n",
    "                            \"created_at\": dt.isoformat(),\n",
    "                            \"hour\": dt.hour,\n",
    "                            \"day_of_week\": dt.strftime(\"%A\"),\n",
    "                            \"is_weekend\": dt.weekday() >= 5,\n",
    "                            \"source\": tweet.get(\"source\", \"\"),\n",
    "                            \"text_length\": len(legacy.get(\"full_text\", \"\")),\n",
    "                            \"num_hashtags\": len(legacy[\"entities\"].get(\"hashtags\", [])),\n",
    "                            \"num_mentions\": len(legacy[\"entities\"].get(\"user_mentions\", [])),\n",
    "                            \"num_urls\": len(legacy[\"entities\"].get(\"urls\", [])),\n",
    "                            \"has_media\": \"media\" in legacy[\"entities\"],\n",
    "                            \"likes\": legacy.get(\"favorite_count\", 0),\n",
    "                            \"retweets\": legacy.get(\"retweet_count\", 0),\n",
    "                            \"replies\": legacy.get(\"reply_count\", 0),\n",
    "                            \"quotes\": legacy.get(\"quote_count\", 0),\n",
    "                            \"views\": int(tweet.get(\"views\", {}).get(\"count\", 0)),\n",
    "                            \"followers\": tweet[\"core\"][\"user_results\"][\"result\"][\"legacy\"][\"followers_count\"],\n",
    "                        }\n",
    "\n",
    "                        tweet_features[\"engagement_rate\"] = (\n",
    "                            tweet_features[\"likes\"] + tweet_features[\"retweets\"] + tweet_features[\"replies\"]\n",
    "                        ) / max(tweet_features[\"followers\"], 1)\n",
    "\n",
    "                        all_tweets.append(tweet_features)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping tweet due to error: {e}\")\n",
    "                        continue\n",
    "\n",
    "    return pd.DataFrame(all_tweets)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2545df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "input_path = \"data/raw/tweets_data.json\"\n",
    "output_path = \"data/processed/tweets_engagement_dataset.csv\"\n",
    "\n",
    "# Load and extract\n",
    "data = load_data(input_path)\n",
    "df = extract_tweet_features(data)\n",
    "\n",
    "# Sort per influencer by timestamp\n",
    "df.sort_values(by=[\"user_id\", \"created_at\"], inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Extracted dataset saved to {output_path}\")\n",
    "\n",
    "# Parse datetime\n",
    "df[\"created_at\"] = pd.to_datetime(df[\"created_at\"])\n",
    "\n",
    "# Define latest and 6 months ago\n",
    "latest = df[\"created_at\"].max()\n",
    "six_months_ago = latest - pd.DateOffset(months=6)\n",
    "\n",
    "# üõ† Only localize if not already tz-aware\n",
    "if six_months_ago.tzinfo is None and latest.tzinfo is not None:\n",
    "    six_months_ago = six_months_ago.tz_localize(latest.tzinfo)\n",
    "\n",
    "# Filter\n",
    "df_recent = df[df[\"created_at\"] >= six_months_ago]\n",
    "\n",
    "# Output\n",
    "print(\"Latest tweet:\", latest)\n",
    "print(\"6 months ago:\", six_months_ago)\n",
    "print(f\"Filtered tweets count: {len(df_recent)}\")\n",
    "print(f\"Earliest filtered tweet: {df_recent['created_at'].min()}\")\n",
    "print(f\"Latest filtered tweet: {df_recent['created_at'].max()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a4e55",
   "metadata": {},
   "source": [
    "## Social Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ebaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Social Network Construction (Real Data)\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the collected data\n",
    "with open('data/raw/influencer_profiles.json', 'r') as f:\n",
    "    influencer_profiles = json.load(f)\n",
    "\n",
    "with open('data/raw/following_data.json', 'r') as f:\n",
    "    following_data = json.load(f)\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Store influencer nodes with their metadata\n",
    "influencer_nodes = []\n",
    "user_id_map = {}  # screen_name.lower() ‚Üí user_id\n",
    "screen_name_map = {}  # user_id ‚Üí screen_name.lower()\n",
    "\n",
    "# Add influencer nodes to graph\n",
    "for profile in influencer_profiles:\n",
    "    try:\n",
    "        user_data = profile['result']['data']['user']['result']\n",
    "        user_id = user_data['rest_id']\n",
    "        screen_name = user_data['legacy']['screen_name']\n",
    "        name = user_data['legacy']['name']\n",
    "        followers_count = user_data['legacy']['followers_count']\n",
    "        friends_count = user_data['legacy']['friends_count']\n",
    "        statuses_count = user_data['legacy']['statuses_count']\n",
    "\n",
    "        G.add_node(user_id,\n",
    "                   screen_name=screen_name,\n",
    "                   name=name,\n",
    "                   followers_count=followers_count,\n",
    "                   friends_count=friends_count,\n",
    "                   statuses_count=statuses_count,\n",
    "                   is_influencer=True)\n",
    "\n",
    "        influencer_nodes.append({\n",
    "            'user_id': user_id,\n",
    "            'screen_name': screen_name,\n",
    "            'name': name,\n",
    "            'followers_count': followers_count,\n",
    "            'friends_count': friends_count,\n",
    "            'statuses_count': statuses_count\n",
    "        })\n",
    "\n",
    "        user_id_map[screen_name.lower()] = user_id\n",
    "        screen_name_map[user_id] = screen_name.lower()\n",
    "\n",
    "    except (KeyError, TypeError) as e:\n",
    "        print(f\"Error processing profile: {e}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "influencer_df = pd.DataFrame(influencer_nodes)\n",
    "\n",
    "# Extract real following relationships to build edges\n",
    "def extract_following_ids(entries):\n",
    "    user_ids = set()\n",
    "    for entry in entries:\n",
    "        try:\n",
    "            content = entry['content']['itemContent']\n",
    "            followed_id = content['user_results']['result']['rest_id']\n",
    "            user_ids.add(followed_id)\n",
    "        except (KeyError, TypeError):\n",
    "            continue\n",
    "    return user_ids\n",
    "\n",
    "# Create edges: source ‚Üí target if source follows target and both are influencers\n",
    "influencer_ids = set(screen_name_map.keys())\n",
    "\n",
    "for item in following_data:\n",
    "    source_id = item['user_id']\n",
    "    instructions = item.get('following', {}).get('result', {}).get('timeline', {}).get('instructions', [])\n",
    "\n",
    "    # Find the 'TimelineAddEntries' section with actual data\n",
    "    for instruction in instructions:\n",
    "        if instruction.get('type') == 'TimelineAddEntries':\n",
    "            entries = instruction.get('entries', [])\n",
    "            followed_ids = extract_following_ids(entries)\n",
    "\n",
    "            for target_id in followed_ids:\n",
    "                if target_id in influencer_ids:\n",
    "                    G.add_edge(source_id, target_id, weight=1)\n",
    "\n",
    "# Visualize the network\n",
    "plt.figure(figsize=(12, 8))\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "# Draw nodes\n",
    "node_sizes = [G.nodes[n]['followers_count'] / 1_000_000 for n in G.nodes()]\n",
    "nx.draw_networkx_nodes(G, pos,\n",
    "                       node_size=[max(5, size * 20) for size in node_sizes],\n",
    "                       node_color='skyblue',\n",
    "                       alpha=0.8)\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.4)\n",
    "\n",
    "# Draw labels\n",
    "labels = {node: G.nodes[node]['screen_name'] for node in G.nodes()}\n",
    "nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "\n",
    "plt.title(\"Social Network of Top Influencers\")\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/outputs/visualizations/influencer_network.png', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Save the graph\n",
    "nx.write_gexf(G, 'data/processed/influencer_network.gexf')\n",
    "\n",
    "# Display stats\n",
    "print(\"Network Statistics:\")\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(f\"Network density: {nx.density(G):.4f}\")\n",
    "\n",
    "# Display influencer info\n",
    "print(\"\\nInfluencer Summary:\")\n",
    "print(influencer_df[['name', 'screen_name', 'followers_count']].sort_values(by='followers_count', ascending=False))\n",
    "\n",
    "print(\"\\nSocial network construction (real data) completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9c02f",
   "metadata": {},
   "source": [
    "## Social Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3faac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Social Network Analysis\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the influencer network graph\n",
    "G = nx.read_gexf('data/processed/influencer_network.gexf')\n",
    "\n",
    "# === 1. CENTRALITY ANALYSIS ===\n",
    "# Compute centrality metrics\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "# Combine into DataFrame\n",
    "centrality_df = pd.DataFrame({\n",
    "    'user_id': list(G.nodes()),\n",
    "    'degree_centrality': list(degree_centrality.values()),\n",
    "    'betweenness_centrality': list(betweenness_centrality.values()),\n",
    "    'closeness_centrality': list(closeness_centrality.values()),\n",
    "    'eigenvector_centrality': list(eigenvector_centrality.values()),\n",
    "    'pagerank': list(pagerank.values())\n",
    "})\n",
    "\n",
    "# Add influencer attributes from the graph\n",
    "for i, node in enumerate(centrality_df['user_id']):\n",
    "    centrality_df.loc[i, 'name'] = G.nodes[node]['name']\n",
    "    centrality_df.loc[i, 'screen_name'] = G.nodes[node]['screen_name']\n",
    "    centrality_df.loc[i, 'followers_count'] = float(G.nodes[node]['followers_count'])\n",
    "    centrality_df.loc[i, 'friends_count'] = float(G.nodes[node]['friends_count'])\n",
    "    centrality_df.loc[i, 'statuses_count'] = float(G.nodes[node]['statuses_count'])\n",
    "\n",
    "# === 2. INFLUENCER SCORE ===\n",
    "# Normalize centrality + followers data\n",
    "scaler = StandardScaler()\n",
    "features = ['degree_centrality', 'betweenness_centrality', \n",
    "            'closeness_centrality', 'eigenvector_centrality', \n",
    "            'pagerank', 'followers_count']\n",
    "scaled_centrality = scaler.fit_transform(centrality_df[features])\n",
    "\n",
    "# Weighted score\n",
    "weights = [0.15, 0.10, 0.10, 0.25, 0.15, 0.25]\n",
    "influencer_scores = np.dot(scaled_centrality, np.array(weights))\n",
    "centrality_df['influencer_score'] = influencer_scores\n",
    "\n",
    "# === 3. COMMUNITY DETECTION BASED ON GRAPH STRUCTURE ===\n",
    "try:\n",
    "    import community as community_louvain  # Make sure you have 'python-louvain' installed\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please install it via pip: pip install python-louvain\")\n",
    "\n",
    "# Convert to undirected for better community detection\n",
    "G_undirected = G.to_undirected()\n",
    "\n",
    "# Run Louvain algorithm on the graph (structure-based)\n",
    "partition = community_louvain.best_partition(G_undirected, resolution=1.0)\n",
    "\n",
    "# Map communities back to the DataFrame\n",
    "centrality_df['community'] = centrality_df['user_id'].map(partition)\n",
    "\n",
    "# Assign community as node attribute in graph\n",
    "nx.set_node_attributes(G, partition, 'community')\n",
    "\n",
    "print(\"Community detection based on network structure (Louvain) completed.\")\n",
    "\n",
    "\n",
    "# === 4. ANALYSIS OUTPUT ===\n",
    "# Save centrality metrics and scores\n",
    "centrality_df.to_csv('data/outputs/centrality/influencer_centrality.csv', index=False)\n",
    "\n",
    "# === 5. VISUALIZATIONS ===\n",
    "# Network with influencer score + community\n",
    "plt.figure(figsize=(12, 10))\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "node_sizes = [(score + abs(min(influencer_scores)) + 1) * 2000 for score in influencer_scores]\n",
    "node_colors = [centrality_df.loc[centrality_df['user_id'] == node, 'community'].values[0] for node in G.nodes()]\n",
    "\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.7, cmap=plt.cm.Set2)\n",
    "nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)\n",
    "nx.draw_networkx_labels(G, pos, labels={node: centrality_df.loc[centrality_df['user_id'] == node, 'screen_name'].values[0] for node in G.nodes()}, font_size=10)\n",
    "plt.title(\"Influencer Network - Centrality & Community\", fontsize=16)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/outputs/visualizations/influencer_analysis_network.png', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Centrality bar comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "measure_cols = ['degree_centrality', 'betweenness_centrality', 'closeness_centrality', 'eigenvector_centrality', 'pagerank']\n",
    "colors = sns.color_palette(\"husl\", len(measure_cols))\n",
    "sorted_df = centrality_df.sort_values('influencer_score', ascending=False)\n",
    "x = np.arange(len(sorted_df))\n",
    "width = 0.15\n",
    "\n",
    "for i, column in enumerate(measure_cols):\n",
    "    plt.bar(x + i*width, sorted_df[column], width, label=column, color=colors[i])\n",
    "\n",
    "plt.xlabel('Influencers', fontsize=12)\n",
    "plt.ylabel('Centrality Value', fontsize=12)\n",
    "plt.title('Centrality Comparison by Influencer', fontsize=16)\n",
    "plt.xticks(x + width*2, sorted_df['screen_name'], rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/outputs/visualizations/centrality_comparison.png', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Followers vs Influencer Score\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='followers_count', y='influencer_score',\n",
    "                hue='community', size='pagerank', sizes=(100, 1000),\n",
    "                data=centrality_df, palette='Spectral')\n",
    "\n",
    "for i, row in centrality_df.iterrows():\n",
    "    plt.annotate(row['screen_name'], (row['followers_count'], row['influencer_score']),\n",
    "                 xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "plt.title(\"Followers vs Influencer Score\", fontsize=16)\n",
    "plt.xlabel(\"Followers Count\", fontsize=12)\n",
    "plt.ylabel(\"Influencer Score\", fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/outputs/visualizations/followers_vs_score.png', dpi=300)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# === 6. OUTPUT TOP INFLUENCERS AND COMMUNITY STATS ===\n",
    "print(\"Top 5 Influencers Based on Composite Score:\")\n",
    "print(centrality_df[['name', 'screen_name', 'followers_count', 'influencer_score']].sort_values(by='influencer_score', ascending=False).head(5))\n",
    "\n",
    "# print(\"\\nCommunity Group Summary (mean influencer score per group):\")\n",
    "# print(centrality_df.groupby('community')['influencer_score'].mean().reset_index().sort_values(by='influencer_score', ascending=False))\n",
    "\n",
    "print(\"\\nSocial network analysis completed!\")\n",
    "\n",
    "print(\"\\nTop influencers in each community based on influencer score:\")\n",
    "for comm_id in sorted(centrality_df['community'].unique()):\n",
    "    print(f\"\\nCommunity {comm_id}:\")\n",
    "    top_group = centrality_df[centrality_df['community'] == comm_id].sort_values(\"influencer_score\", ascending=False).head(3)\n",
    "    print(top_group[['screen_name', 'followers_count', 'influencer_score']])\n",
    "    \n",
    "num_communities = len(set(partition.values()))\n",
    "print(f\"\\nTotal communities detected: {num_communities}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a73189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load centrality features\n",
    "centrality_path = \"data/outputs/centrality/influencer_centrality.csv\"\n",
    "df_centrality = pd.read_csv(centrality_path)\n",
    "\n",
    "# Ensure both user_id columns are the same type before merging\n",
    "df[\"user_id\"] = df[\"user_id\"].astype(str)\n",
    "df_centrality[\"user_id\"] = df_centrality[\"user_id\"].astype(str)\n",
    "\n",
    "# Now perform the merge\n",
    "df = df.merge(df_centrality, on=\"user_id\", how=\"left\", suffixes=(\"\", \"_centrality\"))\n",
    "\n",
    "\n",
    "# Optional: check for any missing merges\n",
    "missing = df[df[\"degree_centrality\"].isna()]\n",
    "print(f\"Missing centrality data for {len(missing)} tweets.\")\n",
    "print(missing[\"username\"].unique())\n",
    "\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da45ab",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Drop unused column and merge tweet features with centrality features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4fa865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# Drop non-numeric / non-useful columns\n",
    "df = df.drop(columns=[\"tweet_id\", \"created_at\", \"username\", \"screen_name\", \"name\", \"user_id\"])\n",
    "\n",
    "\n",
    "\n",
    "# Convert booleans\n",
    "df[\"has_media\"] = df[\"has_media\"].astype(int)\n",
    "df[\"is_weekend\"] = df[\"is_weekend\"].astype(int)\n",
    "\n",
    "# One-hot encode day_of_week and source\n",
    "df = pd.get_dummies(df, columns=[\"day_of_week\", \"source\"], drop_first=True)\n",
    "\n",
    "# Rename source columns for readability\n",
    "import re\n",
    "new_cols = []\n",
    "for col in df.columns:\n",
    "    if col.startswith(\"source_\"):\n",
    "        match = re.search(r'>(.*?)<', col)\n",
    "        if match:\n",
    "            clean_name = \"source_\" + match.group(1)\n",
    "            new_cols.append(clean_name)\n",
    "        else:\n",
    "            new_cols.append(col)\n",
    "    else:\n",
    "        new_cols.append(col)\n",
    "\n",
    "df.columns = new_cols\n",
    "\n",
    "print(list(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b474794",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, r2_score\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengagement_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengagement_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Split\n",
    "X = df.drop(\"engagement_rate\", axis=1)\n",
    "y = df[\"engagement_rate\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize and train model\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=importances, y=features)\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18fcfe6",
   "metadata": {},
   "source": [
    "## Model Training and Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c308e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"R¬≤ Score:\", r2_score(y_test, y_pred))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df[\"engagement_rate\"], bins=50, kde=True)\n",
    "plt.title(\"Distribution of Engagement Rate\")\n",
    "plt.xlabel(\"Engagement Rate\")\n",
    "plt.ylabel(\"Tweet Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b62cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load with created_at as datetime\n",
    "df_full = pd.read_csv(\"data/processed/tweets_engagement_dataset.csv\", parse_dates=[\"created_at\"])\n",
    "\n",
    "# Copy for visualization and set datetime index\n",
    "df_viz = df_full.copy()\n",
    "df_viz.set_index(\"created_at\", inplace=True)\n",
    "\n",
    "# Calculate cutoff datetime 4 months ago from latest date\n",
    "latest = df_viz.index.max()\n",
    "six_months_ago = latest - pd.DateOffset(months=4)\n",
    "\n",
    "# Filter to keep only last 4 months\n",
    "df_recent = df_viz[df_viz.index >= six_months_ago]\n",
    "\n",
    "# Plot weekly average engagement_rate for last 6 months only\n",
    "df_recent[\"engagement_rate\"].resample(\"W\").mean().plot(title=\"Average Engagement Rate - Last 6 Months\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294cecdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(x='day_of_week', y='engagement_rate', data=df_full)\n",
    "plt.title(\"Avg Engagement Rate by Day of Week\")\n",
    "plt.ylabel(\"Engagement Rate\")\n",
    "plt.xlabel(\"Day of Week\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf51e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Create a DataFrame to compare actual vs predicted\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual\": y_test,\n",
    "    \"Predicted\": y_pred\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "print(results_df.head(10))  # Show first 10 rows\n",
    "\n",
    "# Plot actual vs predicted\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(results_df[\"Actual\"], label=\"Actual\", marker='o')\n",
    "plt.plot(results_df[\"Predicted\"], label=\"Predicted\", marker='x')\n",
    "plt.title(\"Actual vs Predicted Engagement Rate\")\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Engagement Rate\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b9e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(results_df[\"Actual\"], results_df[\"Predicted\"], alpha=0.5)\n",
    "plt.plot([results_df[\"Actual\"].min(), results_df[\"Actual\"].max()],\n",
    "         [results_df[\"Actual\"].min(), results_df[\"Actual\"].max()],\n",
    "         color='red', linestyle='--')  # perfect prediction line\n",
    "plt.xlabel(\"Actual Engagement Rate\")\n",
    "plt.ylabel(\"Predicted Engagement Rate\")\n",
    "plt.title(\"Actual vs Predicted Engagement Rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec3ec7",
   "metadata": {},
   "source": [
    "## Strategy Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/raw/tweets_data.json', 'r') as f:\n",
    "    tweets = json.load(f)\n",
    "\n",
    "rows = []\n",
    "for user in tweets:\n",
    "    instructions = user.get('tweets', {}).get('result', {}).get('timeline', {}).get('instructions', [])\n",
    "    for instruction in instructions:\n",
    "        if instruction.get('type') == 'TimelineAddEntries':\n",
    "            entries = instruction.get('entries', [])\n",
    "            for entry in entries:\n",
    "                content = entry.get('content', {})\n",
    "                item_content = content.get('itemContent', {})\n",
    "                tweet_result = item_content.get('tweet_results', {}).get('result', {})\n",
    "                legacy = tweet_result.get('legacy', {})\n",
    "                text = legacy.get('full_text', \"\")\n",
    "                timestamp = legacy.get('created_at', None)\n",
    "                if text and timestamp:\n",
    "                    try:\n",
    "                        dt = datetime.strptime(timestamp, '%a %b %d %H:%M:%S %z %Y')\n",
    "                        rows.append({\n",
    "                            'text': text,\n",
    "                            'hour': dt.hour,\n",
    "                            'likes': legacy.get('favorite_count', 0),\n",
    "                            'retweets': legacy.get('retweet_count', 0)\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Skipping tweet due to error: {e}\")\n",
    "\n",
    "df_tweets = pd.DataFrame(rows)\n",
    "print(f\"‚úÖ Loaded {len(df_tweets)} tweets with timestamps.\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# PART C: Optimal Posting Time\n",
    "# ---------------------------------------\n",
    "\n",
    "hourly_likes = df_tweets.groupby('hour')['likes'].mean()\n",
    "best_hour = hourly_likes.idxmax()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "hourly_likes.plot(kind='bar', color='skyblue')\n",
    "plt.title(\"Avg Likes by Posting Hour\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Average Likes\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/outputs/visualizations/engagement_by_hour.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\"üïí Best posting hour based on avg likes: {best_hour}:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f6066",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab687f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# PART D: Sentiment Analysis\n",
    "# ---------------------------------------\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "df_tweets['sentiment'] = df_tweets['text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df_tweets['sentiment'], bins=30, kde=True, color='orange')\n",
    "plt.title(\"üß† Tweet Sentiment Distribution\")\n",
    "plt.xlabel(\"Sentiment Polarity\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/outputs/visualizations/sentiment_distribution.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "sentiment_corr = df_tweets[['sentiment', 'likes', 'retweets']].corr()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(sentiment_corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Sentiment vs Engagement Correlation\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/outputs/visualizations/sentiment_engagement_correlation.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14fd014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# PART E: Topic Modeling & Engagement\n",
    "# ---------------------------------------\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
    "X_topics = vectorizer.fit_transform(df_tweets['text'])\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X_topics)\n",
    "\n",
    "df_tweets['topic'] = lda.transform(X_topics).argmax(axis=1)\n",
    "topic_engagement = df_tweets.groupby('topic')[['likes', 'retweets']].mean().sort_values(by='likes', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "topic_engagement.plot(kind='bar', stacked=True)\n",
    "plt.title(\"Engagement by Tweet Topic\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.ylabel(\"Average Engagement\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/outputs/visualizations/engagement_by_topic.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüóÇ Top Keywords per Topic:\")\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    top_keywords = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]\n",
    "    print(f\"Topic {idx}: {', '.join(top_keywords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d577a50d",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fdf0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# PART G: Strategy Recommendations\n",
    "# ---------------------------------------\n",
    "\n",
    "print(\"\\n‚úÖ Strategy Recommendations Based on Data:\")\n",
    "print(f\"‚Ä¢ üïí Optimal posting time: {best_hour}:00\")\n",
    "\n",
    "print(\"‚Ä¢ üî• Topics with highest average engagement:\")\n",
    "top_topics = topic_engagement.head(3)\n",
    "for i, row in top_topics.iterrows():\n",
    "    print(f\"   - Topic {i} ‚Üí Avg Likes: {row['likes']:.1f}, Retweets: {row['retweets']:.1f}\")\n",
    "\n",
    "print(\"‚Ä¢ üß† Sentiment correlation with likes: {:.2f}\".format(sentiment_corr.loc['sentiment', 'likes']))\n",
    "print(\"‚Ä¢ üîÅ Sentiment correlation with retweets: {:.2f}\".format(sentiment_corr.loc['sentiment', 'retweets']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wif3009",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
